{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Install txtai and elasticsearch python client\n!pip install git+https://github.com/neuml/txtai elasticsearch\n\n# Download and extract elasticsearch\n!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.8.1-linux-x86_64.tar.gz\n!tar -xzf elasticsearch-7.8.1-linux-x86_64.tar.gz\n!chown -R daemon:daemon elasticsearch-7.8.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfrom subprocess import Popen, PIPE, STDOUT\n\n# Start and wait for server\nserver = Popen(['elasticsearch-7.8.1/bin/elasticsearch'], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1))\nfrom elasticsearch import Elasticsearch\nes = Elasticsearch(hosts=[\"http://localhost:9200\"], timeout=60, retry_on_timeout=True)\nes.indices.create(index=\"environment\", ignore=400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lines=[]\nwith open('../input/combined/tvnews_corpus.tsv', 'r', errors='replace') as f:\n    lines = f.readlines()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport csv\ncounter = 1\nfinalMapping = dict()\n\nfor dirname,_,filenames in os.walk('../input/environmental-news-nlp-dataset/TelevisionNews'):\n    for filename in tqdm(filenames, \"progress\"):\n        path = os.path.join(dirname, filename)\n        with open(path, 'r', errors='replace') as file: \n            reader = csv.reader(file)\n            dictForRow = dict()\n            rowNum = -1\n            for row in reader:\n                if(rowNum!=-1):\n                    rowNum += 1\n                    dictForRow['snippet'] = row[6]\n                    strBuilder = str(filename) + \"#\" + str(rowNum)\n                    finalMapping[counter] = strBuilder\n                    es.index(index=\"environment\", doc_type=\"env\", id=strBuilder, body=dictForRow)\n                    counter += 1\n                rowNum+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = es.search(index=\"environment\", body={\"from\":0, \"size\":10000, \"min_score\":0, \"query\":{\"match\":{\"snippet\":\"Global warming is a hoax\"}}})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pywsd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nimport operator\nimport time\nfrom pywsd.utils import lemmatize_sentence\nfrom numpy import dot\nfrom numpy.linalg import norm\n\ndef cosine_similarity(v1,v2):\n    return dot(v1, v2)/(norm(v1)*norm(v2))\n\n# The inverted index file contents are loaded into memory\n# All queries will be analyzed with this, so it is fine to preload\nwith open(\"../input/indexing/inv_index_percsv_version\",\"rb\") as f:\n    posting_list = pickle.load(f)\n    file_dict = pickle.load(f)\n    vocab = pickle.load(f)\n\n# The actual data is also loaded to be display the search results\nfileobj = open('../input/combined/tvnews_corpus.tsv','r', errors='ignore')\nlines = fileobj.readlines()\nfileobj.close()\n\ndef searchAPI(searchterm):\n    query = searchterm\n    query_tokens = lemmatize_sentence(query) # lemmatize tokens to use as in vocabulary\n    query_vector = []\n    query_tf = {}\n    total_query_vocab = 0\n    for tok in query_tokens:\n        try:\n            indexvalue = vocab.index(tok)\n            query_vector.append(indexvalue)\n            query_tf[indexvalue] = 1 + query_tf.get(indexvalue,0)\n            total_query_vocab += 1\n        except ValueError: # Token doesnt exist in vocab - ignored\n            deadcode='a'\n\n\n    start_time = time.time() # Timer starts\n\n    # First we obtain the list of all possible documents we actually need to search\n    # This is a union of the docs in each query term's posting list\n    # Not an intersection because we use cosine similarity and not boolean retrieval\n    possible_docs = set()\n    query_tf_vector = []\n\n    for q in query_vector:\n        possible_docs = possible_docs.union(posting_list[q].keys())\n        query_tf_vector.append(query_tf[q]/total_query_vocab)\n        # We also generate a TDF vector for the query. Does not make sense to scale with IDF\n\n    # Run through each doc and generate the vector corresponding to the query terms\n    # Compute the cosine similarities of it vs the TF vector of the query\n    # Ties are broken by the magnitude of the vector - note that this is obtained by only considering the query terms\n    # Plus these query term weights were scaled with relative TF, so a higher magnitude means the terms were more important\n    doc_scores = {}\n    for doc in possible_docs:\n        doc_vector = []\n        for q in query_vector:\n            doc_vector.append(posting_list[q].get(doc,0))\n        doc_scores[doc] = (cosine_similarity(doc_vector,query_tf_vector), norm(doc_vector))\n\n    # Results are sorted\n    sorted_results = sorted(doc_scores.items(), key=operator.itemgetter(1), reverse=True)\n\n    end_time = time.time() # Timer ends as search portion is complete\n    search_time = end_time - start_time\n\n    ct = 1\n    #print(\"-------------- SEARCH RESULTS --------------\")\n    results={}\n    results['Details']=[]\n    results['Documents']=[]\n    for i in sorted_results:\n        fname, rownum = file_dict[i[0]].split(' ')\n        rownum = int(rownum[3:])\n        search_res = lines[i[0]]\n        search_res = search_res.split('\\t')[2]\n        answer=fname+'#'+str(rownum+1)\n        results['Documents'].append(answer.split('\\\\')[-1])\n        results['Details'].append({'Name': fname+'#'+str(rownum+1), 'Score': i[1], 'Results': search_res})\n        ct += 1\n    results['Time']=end_time-start_time\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def retrivetopkelastic(hits):\n    topk=[]\n    for i in range(0,len(hits)):\n        topk.append(hits[i]['_id'])\n    return topk\n\ndef retrievaAllElastic(line):\n    total=[]\n    data = es.search(index=\"environment\",scroll='2m', body={\"min_score\":0, \"query\":{\"match\":{\"snippet\":line}}})\n    sid = data['_scroll_id']\n    scroll_size = len(data['hits']['hits'])\n    while scroll_size > 0:\n        scroll_size = len(data['hits']['hits'])\n        total+=[id['_id'] for id in data['hits']['hits']]\n        data = es.scroll(scroll_id=sid, scroll='2m')\n        sid = data['_scroll_id']\n    return total","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport time\n\ntestcasefile=\"../input/samplequeries/sample_queries.txt\"\ninputfile= open(testcasefile,\"r\")\ntimelist1=[]\ntimelist2=[]\n\nretrieved=[]\nrelevant=[]\n\nfor line in tqdm(inputfile.readlines()[3:5], \"progress\"):\n    \n    #SearchAPI\n    results = searchAPI(line)\n    timelist1.append(results['Time'])\n    retrieved.append(results['Documents'])\n    \n    #Elastic Search\n    tic=time.time()\n    relevant.append(retrievaAllElastic(line))\n    toc=time.time()\n    timelist2.append(toc-tic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"resultsperindex9queryfinal\",\"wb\") as f:\n    pickle.dump(retrieved,f)\n    pickle.dump(relevant, f)\n    pickle.dump(timelist1,f)\n    pickle.dump(timelist2,f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precisionnum = 0\nprecisionden = 0\nfor i in range(10):\n    precisionnum+=len(set(retrieved[i][:1]).intersection(relevant[i]))\n    precisionden+=len(set(retrieved[i][:1]))\nprint(precisionnum/precisionden)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a href=\"./resultsperindex9queryfinal\"> Download File </a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}